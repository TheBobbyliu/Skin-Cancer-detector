{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 癌症检测\n",
    "\n",
    "#### 流程：\n",
    "\n",
    "1. 数据下载与预处理，得到图片为224* 224* 3大小，标准化，并且将数据类别进行one-hot编码\n",
    "2. 数据增强\n",
    "3. 权重初始化\n",
    "4. L2正则化\n",
    "5. 建立神经网络，使用VGG16，先训练bottleneck features，再将前面的卷积层加入一起训练。\n",
    "6. 绘制损失曲线，并且可视化第一层的权重。\n",
    "7. 将数据分为5份，交叉验证。\n",
    "\n",
    "尝试：fancy PCA，数据预训练（无监督）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file already exists!\n",
      "Validation file already exists!\n",
      "Test file already exists!\n"
     ]
    }
   ],
   "source": [
    "# download files\n",
    "from os.path import isdir, isfile\n",
    "from os import mkdir\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlretrieve\n",
    "import urllib\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "    def hook(self, block_num = 1, block_size = 1, total_size = None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num-self.last_block)*block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0'}  \n",
    "if not isfile('./train.zip'):\n",
    "    with DLProgress(unit = 'B',unit_scale = True, miniters=1, desc = 'train.zip') as pbar:        \n",
    "        url = 'https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/v4-dataset/train.zip'\n",
    "        req = urllib.request.Request(url=url, headers=headers)  \n",
    "        urlretrieve(\n",
    "            url,\n",
    "            './train.zip',\n",
    "            pbar.hook)\n",
    "else:\n",
    "    print('Training file already exists!')\n",
    "\n",
    "if not isfile('./valid.zip'):\n",
    "    with DLProgress(unit = 'B',unit_scale = True, miniters=1, desc = 'valid.zip') as pbar:\n",
    "        url = 'https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/v4-dataset/valid.zip'\n",
    "        req = urllib.request.Request(url=url, headers=headers)  \n",
    "        urlretrieve(\n",
    "            url,\n",
    "            './valid.zip',\n",
    "            pbar.hook)\n",
    "else:\n",
    "    print('Validation file already exists!')\n",
    "\n",
    "    \n",
    "if not isfile('./test.zip'):\n",
    "    with DLProgress(unit = 'B',unit_scale= True, miniters=1, desc = 'test.zip') as pbar:\n",
    "        url = 'https://s3.cn-north-1.amazonaws.com.cn/static-documents/nd101/v4-dataset/test.zip'\n",
    "        req = urllib.request.Request(url=url, headers=headers)  \n",
    "        urlretrieve(\n",
    "            url,\n",
    "            './test.zip',\n",
    "            pbar.hook)\n",
    "else:\n",
    "    print('Test file already exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip data\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('./train.zip','r') as f:\n",
    "    for file in tqdm(f.namelist()):\n",
    "        f.extract(file,'./')\n",
    "with zipfile.ZipFile('./valid.zip','r') as f:\n",
    "    for file in tqdm(f.namelist()):\n",
    "        f.extract(file,'./')\n",
    "with zipfile.ZipFile('./test.zip','r') as f:\n",
    "    for file in tqdm(f.namelist()):\n",
    "        f.extract(file,'./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Computing_Software\\anaconda\\envs\\py35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from keras.preprocessing import image\n",
    "from PIL import ImageFile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path, target_size = (224,224))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis = 0)\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensor = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data for future test\n",
    "from keras.utils import np_utils\n",
    "def process_data(filepath):\n",
    "    class_name = [folder for folder in glob(filepath+'/*/') if isdir(folder)]\n",
    "    features = np.array([])\n",
    "    labels = np.array([])\n",
    "    for i in range(len(class_name)):\n",
    "        print(filepath+class_name[i])\n",
    "        filename = class_name[i].replace('\\\\','/')\n",
    "        contents = paths_to_tensor(glob(filename+'/*'))\n",
    "        if features.shape == (0,):\n",
    "            features = contents\n",
    "        else:\n",
    "            features = np.concatenate((features,contents))\n",
    "        if i == 0:\n",
    "            labels = np.zeros((len(contents),1))\n",
    "        else:\n",
    "            add_label = np.array([[i]]*len(contents))\n",
    "            labels = np.concatenate((labels, add_label),axis = 0)\n",
    "        i += 1\n",
    "    labels = labels.flatten()\n",
    "    labels = np_utils.to_categorical(labels)\n",
    "    print('Feature shape: ', features.shape)\n",
    "    print('Label shape:', labels.shape)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./valid./valid\\melanoma\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 30/30 [00:19<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./valid./valid\\nevus\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 78/78 [00:33<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./valid./valid\\seborrheic_keratosis\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 42/42 [00:12<00:00,  3.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape:  (150, 224, 224, 3)\n",
      "Label shape: (150, 3)\n",
      "./test./test\\melanoma\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 117/117 [01:11<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test./test\\nevus\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 393/393 [05:56<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test./test\\seborrheic_keratosis\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 90/90 [00:34<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape:  (600, 224, 224, 3)\n",
      "Label shape: (600, 3)\n"
     ]
    }
   ],
   "source": [
    "from os.path import isdir, isfile\n",
    "from os import mkdir\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlretrieve\n",
    "import urllib\n",
    "train_features,train_labels = process_data('./train')\n",
    "valid_features,valid_labels = process_data('./valid')\n",
    "test_features,test_labels = process_data('./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Standardizer = StandardScaler()\n",
    "for i in range(len(train_features)):\n",
    "    train_features[i] = (train_features[i].astype('float32') - 125.0)/125.0\n",
    "for i in range(len(valid_features)):\n",
    "    valid_features[i] = (valid_features[i].astype('float32') - 125.0)/125.0\n",
    "for i in range(len(test_features)):\n",
    "    test_features[i] = (test_features[i].astype('float32') - 125.0)/125.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    shear_range = 0.2,\n",
    "    fill_mode = 'nearest'\n",
    ")\n",
    "datagen.fit(train_features)\n",
    "datagen.fit(valid_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的初步训练也可以使用EarlyStopping，从而加速训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "# 模型建立\n",
    "from keras.models import Model\n",
    "from keras.applications import vgg16\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# 首先要对模型进行预训练，即固定VGG16前面的权重，对后面部分进行训练\n",
    "model = vgg16.VGG16(weights = 'imagenet',include_top = False, input_shape=(224, 224, 3))\n",
    "print('Model loaded')\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add new layer and regularizers\n",
    "# 版本不符，softmax函数用tf.nn.softmax代替\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "import tensorflow as tf\n",
    "output_shape = train_labels.shape\n",
    "input_shape  = train_features.shape\n",
    "def add_new_layer(model):\n",
    "    x = model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(122, activation = 'relu',kernel_regularizer = regularizers.l1(0.01))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(50, activation = 'relu',kernel_regularizer = regularizers.l1(0.01))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_shape[1], activation = tf.nn.softmax)(x)\n",
    "    model = Model(input = model.input, output = x)\n",
    "    return model\n",
    "def freeze_all_model(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting checkpointer and early stopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "                filepath = 'Cancer_best_weights.hdf5', \n",
    "                verbose=1,\n",
    "                save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Computing_Software\\anaconda\\envs\\py35\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=Tensor(\"in...)`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 122)               3060858   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 122)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                6150      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 17,781,849\n",
      "Trainable params: 3,067,161\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# set up model\n",
    "model= add_new_layer(model)\n",
    "# setting hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "sgd = SGD(lr=learning_rate, decay=1e-4, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer = 'sgd',loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 00001: val_loss improved from inf to 89.74504, saving model to Cancer_best_weights.hdf5\n",
      " - 3244s - loss: 159.0131 - acc: 0.5928 - val_loss: 89.7450 - val_acc: 0.5200\n",
      "Epoch 2/10\n",
      "Epoch 00002: val_loss improved from 89.74504 to 15.30999, saving model to Cancer_best_weights.hdf5\n",
      " - 3087s - loss: 45.5671 - acc: 0.6827 - val_loss: 15.3100 - val_acc: 0.5200\n",
      "Epoch 3/10\n",
      "Epoch 00003: val_loss improved from 15.30999 to 7.35247, saving model to Cancer_best_weights.hdf5\n",
      " - 3412s - loss: 8.5263 - acc: 0.6511 - val_loss: 7.3525 - val_acc: 0.5200\n",
      "Epoch 4/10\n",
      "Epoch 00004: val_loss improved from 7.35247 to 7.21346, saving model to Cancer_best_weights.hdf5\n",
      " - 4215s - loss: 7.0665 - acc: 0.6404 - val_loss: 7.2135 - val_acc: 0.5200\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-d927421bfbab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                     \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                     verbose = 2)\n\u001b[0m",
      "\u001b[1;32mD:\\Computing_Software\\anaconda\\envs\\py35\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Computing_Software\\anaconda\\envs\\py35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2094\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2095\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2096\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2098\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Computing_Software\\anaconda\\envs\\py35\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1812\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1814\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1815\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1816\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Computing_Software\\anaconda\\envs\\py35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2352\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Computing_Software\\anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Computing_Software\\anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Computing_Software\\anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mD:\\Computing_Software\\anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Computing_Software\\anaconda\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model_history = model.fit_generator(datagen.flow(\n",
    "                        train_features,\n",
    "                        train_labels,\n",
    "                        batch_size = batch_size),\n",
    "                    steps_per_epoch = train_features.shape[0]//batch_size,\n",
    "                    callbacks = [checkpointer],\n",
    "                    validation_data=[valid_features, valid_labels],\n",
    "                    epochs = epochs,\n",
    "                    shuffle = True,\n",
    "                    verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-c4c446b7164b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# summarize history for accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# visualize the loss and accuracy to find a perfect point\n",
    "# summarize history for accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始训练完模型后\n",
    "\n",
    "很容易看出，训练过程中虽然验证集损失一直在减少，但其正确率没有得到提高，很可能是因为模型受局限于卷积层固定的权重。\n",
    "\n",
    "这主要是因为我们只训练了最后几层模型，而没有对前面的层进行训练，现在开始结合VGG16的顶层卷积层来训练模型。\n",
    "\n",
    "首先建立网络进行自编码，提取数据更清晰的特征，以防VGG中的卷积层受到噪声的太大影响。\n",
    "\n",
    "下面使用TensorFlow建立一个自编码器autoencoder(在udacity Deep Learning 课程指导下完成)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "inputs_ = tf.placeholder(tf.float32, (None, 224, 224, 3), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32, (None, 224, 224, 3), name='targets')\n",
    "\n",
    "# Encoder\n",
    "conv1 = tf.layers.conv2d(inputs_, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 224x224x16\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1, (2,2), (2,2), padding='same')\n",
    "# 112x112x16\n",
    "conv2 = tf.layers.conv2d(maxpool1, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 112x112x8\n",
    "maxpool2 = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same')\n",
    "# 56x56x8\n",
    "conv3 = tf.layers.conv2d(maxpool2, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 56x56x8\n",
    "encoded = tf.layers.max_pooling2d(conv3, (2,2), (2,2), padding='same')\n",
    "# 28x28x8\n",
    "\n",
    "# Decoder\n",
    "upsample1 = tf.image.resize_nearest_neighbor(encoded, (56,56))\n",
    "# 56x56x8\n",
    "conv4 = tf.layers.conv2d(upsample1, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 56x56x8\n",
    "upsample2 = tf.image.resize_nearest_neighbor(conv4, (112,112))\n",
    "# 112x112x8\n",
    "conv5 = tf.layers.conv2d(upsample2, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 56x56x8\n",
    "upsample3 = tf.image.resize_nearest_neighbor(conv5, (224,224))\n",
    "# 224x224x8\n",
    "conv6 = tf.layers.conv2d(upsample3, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 224x224x16\n",
    "\n",
    "logits = tf.layers.conv2d(conv6, 3, (3,3), padding='same', activation=None)\n",
    "# 224x224x3\n",
    "\n",
    "decoded = tf.nn.sigmoid(logits, name='decoded')\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
    "cost = tf.reduce_mean(loss)\n",
    "opt = tf.train.AdamOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Training loss: 0.6742\n",
      "Epoch: 1/20... Training loss: 0.6763\n",
      "Epoch: 1/20... Training loss: 0.6743\n",
      "Epoch: 1/20... Training loss: 0.6738\n",
      "Epoch: 1/20... Training loss: 0.6715\n",
      "Epoch: 1/20... Training loss: 0.6683\n",
      "Epoch: 1/20... Training loss: 0.6656\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    epochs = 20\n",
    "    batch_size = 200\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for ii in range(len(train_features)//batch_size):\n",
    "            if ii == len(train_features)//batch_size -1:\n",
    "                batch = train_features[-(len(train_features)-(ii+1)*batch_size):]\n",
    "            else:\n",
    "                batch = train_features[ii*batch_size:(ii+1)*batch_size]\n",
    "            batch_cost, _ = sess.run([cost, opt], feed_dict={inputs_: batch,\n",
    "                                                             targets_: batch})\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Training loss: {:.4f}\".format(batch_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results first\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True, figsize=(20,4))\n",
    "imgs = train_features[:10]\n",
    "reconstructed = sess.run(decoded, feed_dict={inputs_: imgs})\n",
    "\n",
    "for images, row in zip([imgs, reconstructed], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.reshape((112, 112)))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "fig.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get results from autoencoder\n",
    "with tf.Session() as sess:\n",
    "    train_features_auto = sess.run(decoded, feed_dict = {inputs_: train_features})\n",
    "    valid_features_auto = sess.run(decoded, feed_dict = {inputs_: valid_features})\n",
    "    test_features_auto = sess.run(decoded, feed_dict = {inputs_: test_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.pooling.MaxPooling2D at 0x1ebcfb00e80>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ebb106ffd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1ebcfcadba8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x1eba84a3048>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x1ebc6445828>,\n",
       " <keras.layers.core.Flatten at 0x1ebc6562c18>,\n",
       " <keras.layers.core.Dense at 0x1ebc6562d68>,\n",
       " <keras.layers.core.Dropout at 0x1ebc65716d8>,\n",
       " <keras.layers.core.Dense at 0x1ebc6571588>,\n",
       " <keras.layers.core.Dropout at 0x1ebc659e978>,\n",
       " <keras.layers.core.Dense at 0x1ebc659eef0>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-11:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "def fine_tune(model):\n",
    "    for layer in model.layers[:-11]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[-11:]:\n",
    "        layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune(model)\n",
    "# setting hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.0001\n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer = 'sgd',loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history_2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
