{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 癌症检测\n",
    "\n",
    "#### 流程：\n",
    "\n",
    "1. 数据下载与预处理，得到图片为224* 224* 3大小，标准化，并且将数据类别进行one-hot编码\n",
    "2. 数据增强\n",
    "3. 权重初始化\n",
    "4. L2正则化\n",
    "5. 建立神经网络，使用VGG16，先训练bottleneck features，再将前面的卷积层加入一起训练。\n",
    "6. 绘制损失曲线，并且可视化第一层的权重。\n",
    "7. 将数据分为5份，交叉验证。\n",
    "\n",
    "尝试：fancy PCA，数据预训练（无监督）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid.zip: 865MB [01:18, 11.0MB/s]                           \n",
      "test.zip: 5.53GB [08:06, 11.4MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "# download files\n",
    "from os.path import isdir, isfile\n",
    "from os import mkdir\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlretrieve\n",
    "import urllib\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "    def hook(self, block_num = 1, block_size = 1, total_size = None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num-self.last_block)*block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0'}  \n",
    "\n",
    "if not isfile('./train.zip'):\n",
    "    with DLProgress(unit = 'B',unit_scale = True, miniters=1, desc = 'train.zip') as pbar:        \n",
    "        url = 'https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/train.zip'\n",
    "        req = urllib.request.Request(url=url, headers=headers)  \n",
    "        urlretrieve(\n",
    "            url,\n",
    "            './train.zip',\n",
    "            pbar.hook)\n",
    "else:\n",
    "    print('Training file already exists!')\n",
    "\n",
    "if not isfile('./valid.zip'):\n",
    "    with DLProgress(unit = 'B',unit_scale = True, miniters=1, desc = 'valid.zip') as pbar:\n",
    "        url = 'https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/valid.zip'\n",
    "        req = urllib.request.Request(url=url, headers=headers)  \n",
    "        urlretrieve(\n",
    "            url,\n",
    "            './valid.zip',\n",
    "            pbar.hook)\n",
    "else:\n",
    "    print('Validation file already exists!')\n",
    "\n",
    "    \n",
    "if not isfile('./test.zip'):\n",
    "    with DLProgress(unit = 'B',unit_scale= True, miniters=1, desc = 'test.zip') as pbar:\n",
    "        url = 'https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/skin-cancer/test.zip'\n",
    "        req = urllib.request.Request(url=url, headers=headers)  \n",
    "        urlretrieve(\n",
    "            url,\n",
    "            './test.zip',\n",
    "            pbar.hook)\n",
    "else:\n",
    "    print('Test file already exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2004/2004 [00:41<00:00, 47.93it/s]\n",
      "100%|██████████| 154/154 [00:06<00:00, 24.53it/s]\n",
      "100%|██████████| 604/604 [00:38<00:00, 15.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# unzip data\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('./train.zip','r') as f:\n",
    "    for file in tqdm(f.namelist()):\n",
    "        f.extract(file,'./')\n",
    "with zipfile.ZipFile('./valid.zip','r') as f:\n",
    "    for file in tqdm(f.namelist()):\n",
    "        f.extract(file,'./')\n",
    "with zipfile.ZipFile('./test.zip','r') as f:\n",
    "    for file in tqdm(f.namelist()):\n",
    "        f.extract(file,'./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/py3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/home/carnd/anaconda3/envs/py3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/carnd/anaconda3/envs/py3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from keras.preprocessing import image\n",
    "from PIL import ImageFile\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "def path_to_tensor(img_path):\n",
    "    img = image.load_img(img_path, target_size = (224,224))\n",
    "    x = image.img_to_array(img)\n",
    "    return np.expand_dims(x, axis = 0)\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensor = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split data for future test\n",
    "from keras.utils import np_utils\n",
    "def process_data(filepath):\n",
    "    class_name = [folder for folder in glob(filepath+'/*/') if isdir(folder)]\n",
    "    features = np.array([])\n",
    "    labels = np.array([])\n",
    "    for i in range(len(class_name)):\n",
    "        print(filepath+class_name[i])\n",
    "        filename = class_name[i].replace('\\\\','/')\n",
    "        contents = paths_to_tensor(glob(filename+'/*'))\n",
    "        if features.shape == (0,):\n",
    "            features = contents\n",
    "        else:\n",
    "            features = np.concatenate((features,contents))\n",
    "        if i == 0:\n",
    "            labels = np.zeros((len(contents),1))\n",
    "        else:\n",
    "            add_label = np.array([[i]]*len(contents))\n",
    "            labels = np.concatenate((labels, add_label),axis = 0)\n",
    "        i += 1\n",
    "    labels = labels.flatten()\n",
    "    labels = np_utils.to_categorical(labels)\n",
    "    print('Feature shape: ', features.shape)\n",
    "    print('Label shape:', labels.shape)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/254 [00:00<00:37,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train./train/seborrheic_keratosis/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:59<00:00,  4.24it/s]\n",
      "  0%|          | 0/1372 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train./train/nevus/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1372/1372 [03:10<00:00,  7.20it/s]\n",
      "  0%|          | 1/374 [00:00<00:56,  6.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./train./train/melanoma/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374/374 [00:55<00:00,  6.79it/s]\n",
      "  2%|▏         | 1/42 [00:00<00:05,  7.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape:  (2000, 224, 224, 3)\n",
      "Label shape: (2000, 3)\n",
      "./valid./valid/seborrheic_keratosis/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:07<00:00,  5.41it/s]\n",
      "  1%|▏         | 1/78 [00:00<00:09,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./valid./valid/nevus/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:21<00:00,  3.70it/s]\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./valid./valid/melanoma/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:12<00:00,  2.47it/s]\n",
      "  1%|          | 1/90 [00:00<00:11,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape:  (150, 224, 224, 3)\n",
      "Label shape: (150, 3)\n",
      "./test./test/seborrheic_keratosis/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:15<00:00,  5.84it/s]\n",
      "  0%|          | 0/393 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test./test/nevus/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 393/393 [03:13<00:00,  2.03it/s]\n",
      "  0%|          | 0/117 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test./test/melanoma/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [00:46<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape:  (600, 224, 224, 3)\n",
      "Label shape: (600, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from os.path import isdir, isfile\n",
    "from os import mkdir\n",
    "from tqdm import tqdm\n",
    "from urllib.request import urlretrieve\n",
    "import urllib\n",
    "train_features,train_labels = process_data('./train')\n",
    "valid_features,valid_labels = process_data('./valid')\n",
    "test_features,test_labels = process_data('./test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the data\n",
    "np.save('train_features.npy',train_features)\n",
    "np.save('train_labels.npy',train_labels)\n",
    "np.save('valid_features.npy',valid_features)\n",
    "np.save('valid_labels.npy',valid_labels)\n",
    "np.save('test_features.npy',test_features)\n",
    "np.save('test_labels.npy',test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# using load command to load the data\n",
    "# this step is not necessary if you have already loaded the data\n",
    "import numpy as np\n",
    "train_features = np.load('train_features.npy')\n",
    "train_labels = np.load('train_labels.npy')\n",
    "valid_features = np.load('valid_features.npy')\n",
    "valid_labels = np.load('valid_labels.npy')\n",
    "test_features = np.load('test_features.npy')\n",
    "test_labels = np.load('test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# standardize data\n",
    "for i in range(len(train_features)):\n",
    "    train_features[i] = (train_features[i].astype('float32') - 125.0)/125.0\n",
    "for i in range(len(valid_features)):\n",
    "    valid_features[i] = (valid_features[i].astype('float32') - 125.0)/125.0\n",
    "for i in range(len(test_features)):\n",
    "    test_features[i] = (test_features[i].astype('float32') - 125.0)/125.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### autoencoder\n",
    "\n",
    "首先建立网络进行自编码，提取数据更清晰的特征。\n",
    "\n",
    "下面使用TensorFlow建立一个自编码器autoencoder。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/py3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/carnd/anaconda3/envs/py3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "inputs_ = tf.placeholder(tf.float32, (None, 224, 224, 3), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32, (None, 224, 224, 3), name='targets')\n",
    "\n",
    "# Encoder\n",
    "conv1 = tf.layers.conv2d(inputs_, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 224x224x16\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1, (2,2), (2,2), padding='same')\n",
    "# 112x112x16\n",
    "conv2 = tf.layers.conv2d(maxpool1, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 112x112x8\n",
    "maxpool2 = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same')\n",
    "# 56x56x8\n",
    "conv3 = tf.layers.conv2d(maxpool2, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 56x56x8\n",
    "encoded = tf.layers.max_pooling2d(conv3, (2,2), (2,2), padding='same')\n",
    "# 28x28x8\n",
    "\n",
    "# Decoder\n",
    "upsample1 = tf.image.resize_nearest_neighbor(encoded, (56,56))\n",
    "# 56x56x8\n",
    "conv4 = tf.layers.conv2d(upsample1, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 56x56x8\n",
    "upsample2 = tf.image.resize_nearest_neighbor(conv4, (112,112))\n",
    "# 112x112x8\n",
    "conv5 = tf.layers.conv2d(upsample2, 8, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 56x56x8\n",
    "upsample3 = tf.image.resize_nearest_neighbor(conv5, (224,224))\n",
    "# 224x224x8\n",
    "conv6 = tf.layers.conv2d(upsample3, 16, (3,3), padding='same', activation=tf.nn.relu)\n",
    "# 224x224x16\n",
    "\n",
    "logits = tf.layers.conv2d(conv6, 3, (3,3), padding='same', activation=None)\n",
    "# 224x224x3\n",
    "\n",
    "decoded = tf.nn.sigmoid(logits, name='decoded')\n",
    "\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
    "cost = tf.reduce_mean(loss)\n",
    "opt = tf.train.AdamOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Training loss: 0.7017\n",
      "Epoch: 1/10... Training loss: 0.6857\n",
      "Epoch: 1/10... Training loss: 0.6771\n",
      "Epoch: 1/10... Training loss: 0.6765\n",
      "Epoch: 1/10... Training loss: 0.6737\n",
      "Epoch: 1/10... Training loss: 0.6731\n",
      "Epoch: 1/10... Training loss: 0.6725\n",
      "Epoch: 1/10... Training loss: 0.6713\n",
      "Epoch: 1/10... Training loss: 0.6694\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    epochs = 10\n",
    "    batch_size = 200\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(epochs):\n",
    "        for ii in range(len(train_features)//batch_size):\n",
    "            if ii == len(train_features)//batch_size -1:\n",
    "                batch = train_features[-(len(train_features)-(ii+1)*batch_size):]\n",
    "            else:\n",
    "                batch = train_features[ii*batch_size:(ii+1)*batch_size]\n",
    "            batch_cost, _ = sess.run([cost, opt], feed_dict={inputs_: batch,\n",
    "                                                             targets_: batch})\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Training loss: {:.4f}\".format(batch_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2a4cf4a12eee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'autoencoder.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "saver = tf.train.Saver(max_to_keep=1)\n",
    "with tf.Session() as sess:\n",
    "    saver.save(sess,'autoencoder.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize the results first\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True, figsize=(20,4))\n",
    "imgs = train_features[:10]\n",
    "reconstructed = sess.run(decoded, feed_dict={inputs_: imgs})\n",
    "\n",
    "for images, row in zip([imgs, reconstructed], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.reshape((112, 112)))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "fig.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get results from autoencoder\n",
    "with tf.Session() as sess:\n",
    "    train_features_auto = sess.run(decoded, feed_dict = {inputs_: train_features})\n",
    "    valid_features_auto = sess.run(decoded, feed_dict = {inputs_: valid_features})\n",
    "    test_features_auto = sess.run(decoded, feed_dict = {inputs_: test_features})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    shear_range = 0.2,\n",
    "    vertical_flip = True,\n",
    "    fill_mode = 'nearest'\n",
    ")\n",
    "datagen.fit(train_features)\n",
    "datagen.fit(valid_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的初步训练也可以使用EarlyStopping，从而加速训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 模型建立\n",
    "from keras.models import Model\n",
    "from keras.applications import vgg16\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# 首先要对模型进行预训练，即固定VGG16前面的权重，对后面部分进行训练\n",
    "model = vgg16.VGG16(weights = 'imagenet',include_top = False, input_shape=(224, 224, 3))\n",
    "print('Model loaded')\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add new layer and regularizers\n",
    "# 版本不符，softmax函数用tf.nn.softmax代替\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "import tensorflow as tf\n",
    "output_shape = train_labels.shape\n",
    "input_shape  = train_features.shape\n",
    "def add_new_layer(model):\n",
    "    x = model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(122, activation = 'relu',kernel_regularizer = regularizers.l1(0.01))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(50, activation = 'relu',kernel_regularizer = regularizers.l1(0.01))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(output_shape[1], activation = tf.nn.softmax)(x)\n",
    "    model = Model(input = model.input, output = x)\n",
    "    return model\n",
    "def freeze_all_model(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setting checkpointer and early stopping\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "                filepath = 'Cancer_best_weights.hdf5', \n",
    "                verbose=1,\n",
    "                save_best_only=True)\n",
    "stopper = EarlyStopping(\n",
    "                monitor = 'val_acc',\n",
    "                patience = 2,\n",
    "                min_delta = 0.0003,\n",
    "                mode = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set up model\n",
    "model= add_new_layer(model)\n",
    "# setting hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "sgd = SGD(lr=learning_rate, decay=1e-4, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer = 'sgd',loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train model\n",
    "model_history = model.fit_generator(datagen.flow(\n",
    "                        train_features,\n",
    "                        train_labels,\n",
    "                        batch_size = batch_size),\n",
    "                    steps_per_epoch = train_features.shape[0]//batch_size,\n",
    "                    callbacks = [checkpointer],\n",
    "                    validation_data=[valid_features, valid_labels],\n",
    "                    epochs = epochs,\n",
    "                    shuffle = True,\n",
    "                    verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the loss and accuracy to find a perfect point\n",
    "# summarize history for accuracy\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始训练完模型后\n",
    "\n",
    "很容易看出，训练过程中虽然验证集损失一直在减少，但其正确率没有得到提高，很可能是因为模型受局限于卷积层固定的权重。\n",
    "\n",
    "这主要是因为我们只训练了最后几层模型，而没有对前面的层进行训练，现在开始结合VGG16的顶层卷积层来训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train \n",
    "def fine_tune(model):\n",
    "    for layer in model.layers[:-11]:\n",
    "        layer.trainable = False\n",
    "    for layer in model.layers[-11:]:\n",
    "        layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fine_tune(model)\n",
    "# setting hyperparameters\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "learning_rate = 0.0001\n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer = 'sgd',loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_history_2 = model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
